#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t broken-links:nil c:nil creator:nil
#+OPTIONS: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+OPTIONS: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:t title:t toc:t todo:t |:t
#+TITLE: Highbrow FP in Rust
#+DATE: <2019-11-02 Sat>
#+AUTHOR: Edward John Steere
#+EMAIL: edward.steere@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 27.0.50 (Org mode 9.1.9)

Now I bet you've seen the title and you think that I'm going to come
up here and *try* to dazzle you with the beauty and elegance of FP.
In truth.  I'm a bit mad.  You see, I've just returned from some
adventuring overseas, with fresh scars and a healthy bout of jet lag,
and when Justin asked me whether I still wanted to go through with the
talk, I said "Sure!  Why not!"

Now it might not sound like such a big deal.  I had a week and a
weekend to put something together for what is usually a very fun and
informal meetup where we all scratch our heads as we try to understand
some arcane detail of the Rust language.

Here's the kicker, though.  Are you ready for it?  So I agreed to do a
talk on Functional Programming in Rust, but I've never actually
written a single Rust program in my life.

So this weekend I went through the mind numbing experience which is
the CCCC.  The Code, Compile, Confusion, Cycle.

I know that most of us are acquainted with the experience already and
it's incredibly odd that I haven't yet been through it, but for those
who haven't and for some laughs for those who have: this is how it
goes:
 - write code in a style which you're familiar with;
 - compile the program and watch as you're assaulted with a slew of
   errors, peppered with helpful advice and hyperlinks;
 - scratch your head in confusion as it claims that the method which
   you're implementing on a trait wont work because the return type,
   which is dependent on the trait, must have known size at compile
   time and that, you could implement a trait which tells the compiler
   about its size.
 - try to use what you've learned along with a healthy dollop of
   reading the docs and then start the cycle again from the coding
   step.

So.  Obviously.  I wouldn't try something crazy like re-implementing
some classic example from the world of FP in Rust.  Would I?  Oh yes I
would.  This talk is as much a story of how that went as it is a
discussion of FP in Rust.  What it does well and why I can't seem to
make it work the way I want it to yet.

So who here has heard of the classic paper "Why Functional Programming
Matters"?  So it makes the case that Functional Programming matters
because it describes a set of characteristics which make it easy and
safe to build bigger programs from small ones and which enable you to
be sure that you're always doing so in a way which will be correct.
The important part here is *correct*.  Functional programmers
generally don't care whether a flock of birds dropping stones onto a
matrix of balloons performs the calculation.  We just want it to be
correct.

So without further ado; let's drop into the story of me bashing my
head against the rust compiler.

So the "Why Functional Programming Matters" paper takes four example
problems and shows how you can implement the solutions to those
problems in a way which builds upon the previous solutions.  The
writers of the paper were so masterful that they made it seem as
though the elements of your program are infinitely divisible.  Into
small little pieces which are all correct and useful in isolation, and
which can be used in conjunction with each other to build a bigger
program.

Let's take a look at the first one.  Everyone heard of Newton's
approximation to finding the square root of a number?  Well apparently
there was another dude who should be credited when we talk about the
method (Rhapson)?

Anyway, here's the formula for computing a better approximation from
your current approximation.  Repeated application of this formula will
result in an improving series of approximations to the square root of
a number.  So lets take a look at how you'd go about implementing a
numerical square root calculator using this function.

Here's a standard imperative solution to the problem.  We ask for an
epsilon because we don't want global variables hanging over us.  And
we repeatedly apply the formula until the ratio of the next
approximation to the previous is good enough.  You can also use the
absolute difference between two successive values, but this also has
some pitfalls.  (You might, for instance, never terminate with that
method because the approximation might not converge within your
desired accuracy.)

This method works fine.  We can find pretty accurate approximations of
the square root of a number.  Check it out.  Here's the square root of
five and it does indeed approximately square back to 5.

If we take a look at the code for the imperative square root
calculator again we can see that there are a number of useful elements
in the code.  We have:
 - a heuristic for determining when to stop iterating;
 - a calculation of the next approximation which is dependent on the
   previous;
 - the implementation of a control structure which iterates until the
   approximation is good enough;

If you're of the opinion that these elements should not be
disentangled then I'm hoping to make you reconsider your position
before the night is over.

Now it's time for a bit of a tangent, because, wow, did I bang my head
against Rust for a while.

So one of the conclusions which functional programmers made quite
early on is that Laziness makes for good APIs.  Think about
~into_iter~ on a collection and the way that you can chain operations
on iterators.  This API is so powerful that most Rust programs can be
expressed almost entirely in this style.  Pretty cool stuff(!)

I needed something slightly different though, because I'm looking to
keep scanning through entries *until* I find one which satisfies some
properties.  Namely that the approximation is good enough.  Usually
you work with iterators off of fixed size containers and then convert
them back into fixed size containers in the end.  I took a look at the
API for ~Iterator~ and it was confusing.  So much so that I decided
after looking through the docs that it couldn't possibly handle the
cases which I needed it to.

So I set about implementing my own iterator.  Here it is.  It
specifies that things which can iterate should be able to construct
themselves from a value (presumably the initial value and perhaps a
bad choice, but I'll get back to that soon).  You should be able to
advance the iteration by one step and you should be able to get the
current value of the iterator.

It relies on a few powerful Rust constructs and I'd like to
spend a moment going through each one.

Firstly, we've all seen traits, but have you seen the Self type?  This
is used to refer to the type of the struct which implements the
trait so that the compiler can know the size of the return type.  To
understand this a bit more consider that we'd instead said that ~next~
produces an ~Iterable<T>~.  Now the compiler will complain, because
the size of ~Iterable<T>~ cannot be known at compile time.

This is the biggest issue which I bumped my head into while I was
working on this problem.  Apparently you can implement a bound trait
to tell the compiler about the size of the return type, but I never
got around to looking at that.

Next, lets take a look at my implementation of ~take~.  The
implementation is pretty straight forward.  I've used fold, but I
really should have used ~for_each~.  Unfortunately Rust doesn't have
persistent data structures in its standard library so you're stuck
pushing onto a vector and returning it, but that's not the end of the
world because this function looks pure from the outside anyway.

So this function uses a feature which I think will enable Rust to go
much, much further into the world of FP.  Existentially quantified
types.  That's an intimidating name for a feature in a type system to
have, but, like most mathematical ideas, the name is more intimidating
than the thing itself.

Who can spot the existentially quantified type?  It's over there!
Some may know this as "abstract types" and that's possibly a better
name for programmers who were weaned off of ~C~ with OOP.  What this
essentially means is that there exists some type for which this
function must work.  Basically put: compiler, this must work for any
type which is an ~Iterable<T>~, so when I use it that way, that's when
you can determine the memory constraints etc.

Here's another one which uses existentially quantified types, and this
time I'm using the ~impl~ keyword in the argument position and in the
return type.  This time the fold is more appropriate because I'm
interested in the iterator when I've taken a few elements off of it.

If you're looking at this code and wondering why I didn't just use a
Trait Object then the answer is simple.  ~impl~ is not boxed.  Pretty
darn impressive right?  No V-Table lookup or anything(!)  All just
static dispatch.

Now for a puzzle of sorts: why doesn't this work?  I'll post it online
so that you can take a look afterwards, but it's not an easy problem
to solve.  I couldn't solve it over the weekend, maybe you'll have
more luck!

Alright!  So that's my re-implementation of iterators.  Just as a
reminder: I did this because I didn't think that Rust supported
infinite iteration.  It turns out that it does and we'll cover that in
a minute, but first let's get back to the Newton-Rhapson approximation
for square roots.

Firstly I need a place to store the number which is being square
rooted and the current approximation.  Then I need to specify how to
advance the iteration.  All pretty straight forward stuff, but already
we've pulled two interesting pieces out of the seemingly indivisible
imperative solution and created some highly efficient code which
represents the same thing.  We've also hidden the details of
dispatching to the ~next_approximation~ function.

How do we use this?  So we can use ~drop~ to see what it'll be like
after a couple of iterations.  We could also use take to see what the
convergence looks like.

Now how about finding a value which meets our convergence criteria.
These are simple now!  All that those functions have to focus on is
iterating and application of the heuristic.  Unfortunately I couldn't
get an implementation of ~drop_while~ to work as I showed earlier, so
we didn't quite get all the way to an implementation which applies the
heuristic separately from performing the iteration.

So let's see these in action!  As you can see the square root of five
converges pretty quickly.  We can swap between heuristic easily and
the definition of the iterator doesn't change.  So we really have
successfully untangled two pieces of code.

One of the promises I made in the blurb of this presentation was that
we'd get "~C-like~ performance".  "~C-like~ performance" is being
thrown around so much these days that it's fast becoming a buzz
phrase.

Ok.  So let's get some guesses.  Who says that the imperative version
is faster than the iterator version?  Who says that the iterator
version is faster?  Who says that there's no measurable difference?

Here's a simple experiment to tell which one it is.  I didn't expect
these results!  It's pretty much bellow the noise floor, but every
time that I run it, the iterator comes out faster(!)  That's pretty
impressive stuff!

I didn't set out to make something particularly fast.  I really just
wanted to demonstrate how you can glue together smaller pieces to make
a program and how with Functional Programming, seemingly indivisible,
bigger pieces of code can be divided.

This was my very first Rust program!  Naturally I didn't yet know a
lot about the standard library.  So after spending two days bashing my
head against the compiler I eventually discovered how to do this stuff
with built in functionality.  Now let's take a look at the same code,
but implemented with the Rust standard library.

So all of the iterator library that I wrote can be replaced by the
standard iterator ~Successors~ object.  This enables you to describe
infinite iterations and again, all I've told the Rust compiler is that
there exists an ~Iterator~ for which this holds true and it's smart
enough to make this all happen statically.  Here are the
implementations of ~within~ and ~relative~.  Pretty straight forward,
but there's a problem: they mutate their argument.

I'm forced to do this because I want the next element out of the
iterator.  Now in other languages this would mean that these functions
don't compose, because mutating the iterator would mean that the next
function to use it would receive it in an unknown state, but in Rust
you can only have one mutable reference so we can prove that this
function is pure.  The iterator given to this function can't also be
supplied to another function from the calling context.  So even though
it's not strictly pure, we'll never run into impure behaviour(!)

Perhaps it's possible to have your cake and eat it!  But that's only
possible if banging your head against the compiler didn't result in
severe facial injuries!

Let's take a quick look at how you'd go about using the new infinite
iterator.  It's pretty much the same except now you have the full
power of the Rust standard libraries behind you.

This is what I'd like to have done for the implementation of ~within~
because it doesn't even need a mutable reference.  I couldn't get it
to work because it requires that the iterator be borrowed into the
call to zip but also used outside.  I couldn't get ~clone~ or ~copy~
to work either.

So having established the foundation for working with iterators, lets
take a look at a better example.

Now we're trying to differentiate a function at a point.  I went ahead
and copied the implementation of ~within~ and ~relative~ across.

Here's a simple calculation for the slope of a function at a point.
We can successively improve this calculation by choosing a smaller and
smaller ~h~.  Choosing a good value of ~h~ is the challenge.

So we can successively halve a conservative value of ~h~ until the
calculation converges on the slope at point.  Let's model that as a
series of halving values and let's apply it to the slope function.
I've kept these two iterators in the same function, but I could just
as well have defined them separately.

We can use the exact same functions as before to get an approximation
of the slope within an absolute or relative difference to within some
defined precision.  And that's great.  It shows that the method is not
only applicable to square rooting.

To really make my point I'm going to introduce you to the final
component here.  Because we have a series of approximations which
converges on our answer, but it turns out that there's an error term
in those approximations which is proportional to ~h~.

It boils down to this formula.  Where ~n~ can be computed as follows.
So let's take the very same iterator which we can use for either the
~epsilon~ or the ~within~ functions and let's improve the calculation
by eliminating the error term which we introduced with the
approximation.

The series should converge faster now.  Indeed it does.  I peppered
the code with ~println~ so that you can see how many ticks it took to
find the answer.  In this case it's not a massive
improvement... initially.  Let's try improving it again.  This is
getting really unwieldy because I'm not sure how to copy the iterator.
(If any of you know then that'd make my day!)  But it does indeed
improve the convergence.  Now if I could figure out how to copy the
iterator then we could keep on applying improve and it would
eventually converge very quickly.

Let's take a step back though and just think about what we were able
to do.  We took a program which seemed to be indivisible and we
divided it into chunks which were each useful in their own right.
Then we found other ways to glue the same parts of the program
together.  All of this was done in a way which uses truly general
abstractions.  That is, it would be just as valid to model a network
connection with the same sort of ideas that I've presented tonight and
then you'd be able to write small pieces of code and use them to build
bigger programs.

We've done all of this while maintaining not only type safety, but
also, quite miraculously, memory safety in a non garbage collected
context.  These programs are truly fast.  They make Haskell look like
Bash in terms of speed.  I didn't set out to prove that part to you.
I thought that I'd write something which would make us all scratch our
chins and pontificate on the meaning of programming.  I didn't
anticipate that my program would compete with the imperative version.
That is the power of Rust.  If you're willing to bash your head
against a wall for a while, want all the control without losing
safety and can sacrifice a bit of the elegance of Haskell then you'll
never need another programming language.
