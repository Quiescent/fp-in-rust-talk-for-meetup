#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t broken-links:nil c:nil creator:nil
#+OPTIONS: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+OPTIONS: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:t title:t toc:t todo:t |:t
#+TITLE: Highbrow FP in Rust
#+DATE: <2019-11-02 Sat>
#+AUTHOR: Edward John Steere
#+EMAIL: edward.steere@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 27.0.50 (Org mode 9.1.9)

Now I bet you've seen the title and you think that I'm going to come
up here and try to dazzle you with the beauty and elegance of FP.  In
truth.  I'm a bit mad.  You see, I've just returned from some
adventuring overseas, with fresh scars and a healthy bout of jet lag,
and when Justin asked me whether I still wanted to go through with the
talk, I said "Sure!  Why not!"

Now it might not sound like such a big deal.  I had a week and a week
end to put something together for what is usually a very fun and
informal meetup where we all scratch our heads as we try to understand
some arcane detail about the Rust language.

Here's the kicker, though.  Are you ready for it?  So I agreed to do a
talk on Functional Programming in Rust, but I'd never actually written
a single Rust program in my life.

So this weekend I went through the mind numbing experience which is
the CCCC.  The Code, Compile, Confusion, Cycle.

I know that most of us are acquainted with the experience already and
it's incredibly odd that I haven't yet been through it, but for those
who haven't and for some laughs for those who have: this is how it
goes:
 - write code in a style which you're familiar with;
 - compile the program and watch as you're assaulted with a slew of
   errors, peppered with helpful advice and hyperlinks;
 - scratch your head in confusion as it claims that the method which
   you're implementing on a trait wont work because the return type,
   which is dependent on the trait, must have known size at compile
   time and that, you could implement a trait which tells the compiler
   about its size.
 - try to use what you've learned along with a healthy dollop of
   reading the docs and then start the cycle again from the coding
   step.

So.  Obviously.  I wouldn't try something crazy like re-implementing
some classic example from the world of FP in Rust would I?  Oh yes I
would and this talk is as much a story of how that went as it is a
discussion of FP in Rust.  What it does well and why I can't seem to
make it work the way I want it to yet.

So who here has heard of the classic paper "Why Functional Programming
Matters"?  So it makes the case that Functional Programming matters
because it describes a set of characteristics which make it easy and
safe to build bigger programs from small ones and be sure that you're
always doing so in a way which will be correct.  The important part
here is *correct*.  Functional programmers generally don't care
whether a flock of birds dropping stones onto a matrix of balloons
performs the calculation.  We just want it to be correct.

So without further ado; let's drop into the story of me bashing my
head against the rust compiler.

So the "Why Functional Programming Matters" paper takes four example
problems and shows how you can implement the solutions to those
problems in a way which builds upon the previous solutions.  The
writers of the paper were so masterful that they made it seem as
though the elements of your program are infinitely divisible.  Into
small little pieces which are all correct and useful in isolation, and
which can be used in conjunction to build a bigger program.

Let's take a look at the first one.  Everyone heard of Newton's
approximation to finding the square root of a number?  Well apparently
there was another dude who should be credited when we talk about the
method (Rhapson)?

Anyway, here's the formula for computing a better approximation from
your current approximation.  Repeated application of this formula will
result in an improving series of approximations to the square root of
a number.  So lets take a look at how you'd go about implementing a
numerical square root calculator using this function.

Here's a standard imperative solution to the problem.  We ask for an
epsilon because we don't want global variables hanging over us.  And
we repeatedly apply the formula until the ratio of the next
approximation to the previous is good enough.  You can also use the
absolute difference between two successive values, but this is also
has some pitfalls.  (You might, for instance, never terminate with
that method because the approximation might not converge within your
desired accuracy.)

This method works fine.  We can find pretty accurate approximations of
the square root of a number.  Check it out.  Here's the square root of
five and it does indeed approximately square back to 5.

If we take a look at the code for the imperative square root
calculator again we can see that there are a number of useful elements
in the code.  We have:
 - a heuristic for determining when to stop iterating;
 - a calculation of the next approximation which is dependent on the
   previous;
 - the implementation of a control structure which iterates until the
   approximation is good enough;

If you're of the opinion that these elements should not be
disentangled then I'm hoping to make you reconsider your position
before the night is over.

Now it's time for a bit of a tangent, because, wow, did I bang my head
against Rust for a while.

So one of the conclusions which functional programmers made quite
early on is that Laziness makes for good APIs.  Think about
~into_iter~ on a collection and the way that you can chain operations
on iterators.  This API is so powerful that most Rust programs can be
expressed almost entirely in this style.  Pretty cool stuff(!)

I needed something slightly different though, because I'm looking to
keep scanning through entries *until* I find one which satisfies some
properties.  Namely that the approximation is good enough.  Usually
you work with iterators off of fixed size containers and then convert
them back into fixed size containers in the end.  I took a look at the
API for ~Iterator~ and it was confusing.  So much so that I decided
after looking through the docs that it couldn't possibly handle the
cases which I needed it to.

So I set about implementing my own iterator.  Here it is.  It
specifies that things which can iterate should be able to construct
themselves from a value (presumably the initial value and perhaps a
bad choice, but I'll get back to that soon).  You should be able to
advance the iteration by one step

It relies on a few powerful Rust constructs and I'd like to
spend a moment going through each one.

Firstly, we've all seen traits, but have you seen the Self type?  This
is used to refer to the type of the struct which implements the
trait so that the compiler can know the size of the return type.  To
understand this a bit more consider that we'd instead said that ~next~
produces an ~Iterable<T>~.  Now the compiler will complain, because
the size of ~Iterable<T>~ cannot be known at compile time.

This is the biggest issue which I bumped my head into while I was
working on this problem.  Apparently you can implement a bound trait
to tell the compiler about the size of the return type, but I never
got around to looking at that.

Next, lets take a look at my implementation of ~take~.  The
implementation is pretty strait forward.  I've used fold, but I really
should have used ~for_each~.  Unfortunately Rust doesn't have
persistent data structures in its standard library so you're stuck
pushing onto a vector and returning it, but that's not the end of the
world because this function looks pure from the outside anyway.

So this function uses a feature which I think will enable Rust to go
much, much further into the world of FP.  Existentially quantified
types.  That's an intimidating name for a feature in a type system to
have, but, like most mathematical ideas, the name is more intimidating
than the thing itself.

Who can spot the existentially quantified type?  It's over there!
Some may know this as "abstract types" and that's possibly a better
name for programmers who were weaned off of ~C~ with OOP.  What this
essentially means is that there exists some type for which this
function must work.  Basically put: compiler, this must work for any
type which is an ~Iterable<T>~, so when I use it that way, that's when
you can determine the memory constraints etc.

Here's another one which uses existentially quantified types, and this
time I'm using the ~impl~ keyword in the argument position and in the
return type.  This time the fold is more appropriate because I'm
interested in the iterator when I've taken a few elements off of it.

If you're looking at this code and wondering why I didn't just use a
Trait Object then the answer is simple.  ~impl~ is not boxed.  Pretty
darn impressive right?  No V-Table lookup or anything(!)  All just
static dispatch.

Now for a puzzle of sorts: why doesn't this work?  I'll post it online
so that you can take a look afterwards, but it's not an easy problem
to solve.  I couldn't solve it over the weekend, maybe you'll have
more luck!

Alright!  So that's my re-implementation of iterators.  Just as a
reminder: I did this because I didn't think that Rust supported
infinite iteration.  It turns out that it does and we'll cover that in
a minute, but first let's get back to the Newton-Rhapson approximation
for square roots.

Firstly I need a place to store the number which is being square
rooted and the current approximation.  Then I need to specify how to
advance the iteration.  All pretty straight forward stuff, but already
we've pulled two interesting pieces out of the seemingly indivisible
imperative solution and created some highly efficient code which
represents the same thing.  We've also hidden the details of
dispatching to the ~next_approximation~ function.

How do we use this?  So we can use ~drop~ to see what it'll be like
after a couple of iterations.  We could also use take to see what the
convergence looks like.

Now how about finding a value which meets our convergence criteria.
These are simple now!  All that those functions have to focus on is
iterating and application of the heuristic.  Unfortunately I couldn't
get an implementation of ~drop_while~ to work as I showed earlier, so
we didn't quite get all the way to an implementation which applies the
heuristic separately from performing the iteration.

So let's see these in action!  As you can see the square root of five
converges pretty quickly.  We can swap between heuristic easily and
the definition of the iterator doesn't change.  So we really have
successfully untangled two pieces of code.

One of the promises I made in the blurb of this presentation was that
we'd get "~C-like~ performance".  "~C-like~ performance" is being
thrown around so much these days that it's fast becoming a buzz
phrase.

Ok.  So let's get some guesses.  Who says that the imperative version
is faster than the iterator version?  Who says that the iterator
version is faster?  Who says that there's no measurable difference?

Here's a simple experiment to tell which one it is.  I didn't expect
these results!  It's pretty much bellow the noise floor, but every
time that I run it, the iterator comes out faster(!)  That's pretty
impressive stuff!

